{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Oo3JO72loMjhPhYxNGWQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. How is a grayscale image represented on a computer? How about a color image?\n",
        "- Grayscale images are represented as 2-dimmensional array with each element being in the range of 0(White) to 255(Black). Greyscale images only need one channel whereas color images require 3 channels(R/G/B). 3 channels in color images are represented as 2D arrays for each channels. So it becomes rank 3 tensor.  \n",
        "\n",
        "2. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
        "- The files are arranged in two sub folder `train` and `testing`. Training contains data for model training where as `testing` contains data for validation. Validation set are used to offer human readable metric or such as accuracy, error rate. Validation set also make sure that model is not overfitting to the training set by providing metric which can be compared with model on training set. If model performs extremely well during training but not on testing/validation it means that model is overfitting.\n",
        "- Each folder contains two folders for 3 and 7 character whuch contains the .jpg image.\n",
        "\n",
        "3. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
        "- In pixel similarity approach, we generate a mean of single character or archetype for each category. With mnist_sample dataset, we defined the archetype of 3 as pixel wise mean of all images in 3's folder and similarly for 7. Mean image of 3 and 7 can be visualized as blurred versions of 3 and 7. The dark region is where the image pixels agrees that the particular character is represented and blurred region is where the pixel shows low possibility about being that particular character. Later to determine an unseen image(3/7), we calculate the distance to the mean or standard 3 and 7 image. The new image is a 3 if the distace to the archetypical is 3 or lower than to the archetypical 7.\n",
        "\n",
        "4. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
        "- List comprehensios are compact and pythonic way of generating a list using a for loop by writing into a single  expresseion.\n",
        "odd = range(5)\n",
        "odd_double = [o*2 for o in odd]\n",
        "\n",
        "5. What is a \"rank-3 tensor\"?\n",
        "- Rank of a tensor is the number of dimensions it has. Tensor's rank can be identified simply by counting the number or indexes we use to refer the scalar number present inside it. Scalar number can be represented as rank 0 tensor, vector with rank 1, matrix with rank 2 and stack of matrix or cuboid is rank 3 matrix tns[a,b,c]\n",
        "- a tensor of shape 2x2x2 and 2x5x7 are both rank 3 tensor\n",
        "\n",
        "6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
        "- Rank is the number of axes or dimmensions in a tensor.\n",
        "- Shape is the size of each axis of a tensor.\n",
        "- To get the rank of the tensor we can use len(length) on a tensor\n",
        "len(stacked_threes.shapes)\n",
        "- We can also use `.ndim` on tensor which gives us the rank directly\n",
        "stacked_threes.ndim\n",
        "\n",
        "7. What are RMSE and L1 norm?\n",
        "- RMSE and L1 norm are common methods to calculate the 'distnace'\n",
        "- MAE (Mean absolute difference or Absoluse mean error) or L1 norm: Simple method, involves of adding the absolute differences.\n",
        "- RMSE (Root Mean Square Error) or L2 norm: RMSE takes the mean of the square\n",
        "- Simple difference do not work as some differences are positive and some are negative which cancels out each other. Hence, a function on the magnitude of the differences is required to measure the difference.\n",
        "\n",
        "8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
        "- Loops are slow in python, it is best to represent the operations as array operation rather than looping through individual element.\n",
        "- Array operations can be performed with powerful libraries like numpy and pytorch which operates thousands of times faster, as they compiled in C and wrapped in maya. PyTorch offers you even faster operations with the Cuda (C for GPU) running on GPU which are designed specially to run millions of similar operations faster; and hence offer a great boost in performance.\n",
        "\n",
        "9. Create a 3Ã—3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
        "- In: a = torch.Tensor(list(range(1,10))).view(3,3)\n",
        "- In: a\n",
        "- Out: tensor([[1., 2., 3.],\n",
        "               [4., 5., 6.],\n",
        "               [7., 8., 9.]])\n",
        "- In: b = 2*a\n",
        "- In: b\n",
        "- Out: tensor([[ 2.,  4.,  6.],\n",
        "               [ 8., 10., 12.],\n",
        "               [14., 16., 18.]])\n",
        "- In: b[1:,1:]\n",
        "- Out: tensor([[10., 12.],\n",
        "               [16., 18.]])\n",
        "\n",
        "10. What is broadcasting?\n",
        "- Broadcasting is the background process which is used to operate on tensors of different ranks\n",
        "- NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In PyTorch, tensor with smaller rank are expanded to have the same size as the large.\n",
        "- This is how operations are performed between two non similar tensor\n",
        "\n",
        "11. Are metrics generally calculated using the training set, or the validation set? Why?\n",
        "- Metrics are calculated using the validation set.\n",
        "- Validation set is unseen data for the model, evaluating metrics on it is better in order to determine if there is any overfitting and how well the model performs on unseen data (quality to generalize if given similar data)\n",
        "\n",
        "12. What is SGD?\n",
        "- Stochastic Gradient Descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameter of the model in order to minimize the loss function that was evaluated on the prediction and label.\n",
        "- The key idea behind SGD and many other optimization algorithm is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function.\n",
        "\n",
        "13. Why does SGD use mini-batches?\n",
        "- Calculating loss function on one data entry point makes the gradient impresice, rough and unstable which results in incorrect optimizations.\n",
        "- Whereas calculating loss function on entire dataset is not possible due to computational and time limitation.\n",
        "- Alternative and better of both approaches is to make average loss for subset of dataset at a time and these subset are known as mini-batches.\n",
        "- Mini batches are computationally efficient and more reliable than calculating with one data point.\n",
        "14. What are the seven steps in SGD for machine learning?\n",
        "- Initialize the parameters: Set random values in the parameters (weights and bias)\n",
        "- Calculate Prediction: Make predictions on the training set, one mini batch at a time\n",
        "- Calculate the loss: Calculate the average loss of the mini batch\n",
        "- Calculate the gradients: This is an approximation of how the parameters need to change in order to minimize the loss\n",
        "- step the weights: update the parameters based on the weights modified after calculating the gradients\n",
        "- stop: In practice, this is either based on time constraint or usually based on training/validation losses and metrics stop improving.\n",
        "15. How do we initialize the weights in a model?\n",
        "- We initialize the weights in a model by assigning them randomly as they would be updated in training cycle\n",
        "16. What is \"loss\"?\n",
        "- Loss functions returns a value based on the predicions and targets, where lower values corresponds to a better model predictions. This has been the convention used for loss functions in the domain.\n",
        "17. Why can't we always use a high learning rate?\n",
        "- If we use high learning rate then loss may bounce around or oscillate at certain range, as the optimizer is taking large step and updating the parameters faster that it should do to improve the quality of the model\n",
        "18. What is a \"gradient\"?\n",
        "- Gradients tell us how much we have to change each wweight to make our model better. It is essentially a measure of how the loss function changes with the change in the weights of the models(The derivatives)\n",
        "19. Do you need to know how to calculate gradients yourself?\n",
        "- PyTorch offers automatic calculation of the gradients.\n",
        "We have to tag the weights with `requires_grad_()` which could be called later on loss function to return the gradients.\n",
        "20. Why can't we use accuracy as a loss function?\n",
        "- A loss functions needs to change as the weights are being adjusted.\n",
        "- Whereas accuracy only changes if the prediction of model change. So if there's a slight change in the confidence in a prediction, but it does not change the prediction then accuracy would remain same.\n",
        "- Therfore, the gradients will be zero everywhere except when the prediction change. The model therefore cannot learn from the gradient which is zero and model's weight wont be updated leading to not training of the model.\n",
        "- A good loss function would give a slightly better loss when the model gives a better prediction(confidenc). Slightly better predictions means if the model is more confident about the correct prediction. For eg: predicting 0.9 vs 0.7 for probability of MNIST image is a 3 would be slightly better prediction. The loss functions needs to reflect that so that weights could be updated to train the model.\n",
        "21. Draw the sigmoid function. What is special about its shape?\n",
        "- Sigmoid function is smooth curve that squishes all values into values between 0 and 1.\n",
        "- Most loss functions assumes that the model is outputting some form of a probability or confidence level between 0 and 1 so we use a sigmoid function at the ed of the model in order to achieve this.\n",
        "\n",
        "22. What is the difference between a loss function and a metric?\n",
        "- The key difference between loss and metric is that metrics drive human understanding and losses drive automated learning.\n",
        "- In order for loss to be usefull in training, it needs to have a meaningful deriavative.\n",
        "- Many metrics, like accuracy are like that. Metrics are the numbers that humans care about and it reflects the performance of the model\n",
        "23. What is the function to calculate new weights using a learning rate?\n",
        "- The optimization step\n",
        "23. What does the `DataLoader` class do?\n",
        "- DataLoader class can take any Python collection and turn it into an iterator over many batche.\n",
        "24. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
        "- for x, y in dl:\n",
        "pred = model(x)\n",
        "loss = loss_func(pred, y)\n",
        "loss.backward()\n",
        "parameters -= parameters.grad * lr\n",
        "25. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
        "- def func(inputA, inputB): return list(zip(inputA,inputB))\n",
        "This data structure is used in ML where you need a list of tuples where tuple would contain input data and a label.\n",
        "\n",
        "26. What does `view` do in PyTorch?\n",
        "- The view function in PyTorch changes the shape of the tensor without any change in content.\n",
        "\n",
        "27. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
        "- Without the bias parameter, if the input is zero, the output will always be zero. Therefore, using bias parameter adds additional flexibility to a model\n",
        "\n",
        "28. What does the `@` operator do in Python?\n",
        "- @ operator is used to perform matrix multiplication in python\n",
        "\n",
        "29. What does the `backward` method do?\n",
        "- This method returns the current gradient in the loop.\n",
        "\n",
        "30. Why do we have to zero the gradients?\n",
        "- PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, the gradient of the current loss function would be added to the previously stored gradient value.\n",
        "\n",
        "31. What information do we have to pass to `Learner`?\n",
        "- We pass the DataLoaders object, the model, the optimization function, the loss function and optionally any metrics to print.\n",
        "\n",
        "31. Show Python or pseudocode for the basic steps of a training loop.\n",
        "- code\\\n",
        "  def train_epoch(model, lr, params):\n",
        "      for xb, yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        for p in params:\n",
        "          p.data -= p.grad * lr\n",
        "          p.grad.zero_()\n",
        "  for i in range(20):\n",
        "    train_epoch(model, lr, params)\n",
        "\n",
        "32. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
        "- ReLU just means 'replace any negative numbers with zero'. It is a commonly used activation function.\n",
        "\n",
        "33. What is an \"activation function\"?\n",
        "- The acctivation function is another function that is aa part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form `y=wx+b`. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data.\n",
        "- By introducing, non-linearity in between the linear layers, this is no loger true. Each layer is somewhat decoupled from the rest of the layers and the model can now fit more complex functions. In fact, it is mathematically proven that such model can solve any computatable problem to an arbitirarly high accuracy, if the model is large enough with correct weights. This is know as Universal approximation theorem\n",
        "\n",
        "34. What's the difference between `F.relu` and `nn.ReLU`?\n",
        "- F.relu is python function for relu activation function\n",
        "- nn.ReLU is a pythorch module. This means that it is a class and can be called to in same way as python function\n",
        "\n",
        "35. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
        "- There are practical performance benefit of using more than just non-linear function. We can use a deeper model with less number of parameters, better performance, faster training, and compute efficient."
      ],
      "metadata": {
        "id": "d1LnTwth6o97"
      }
    }
  ]
}