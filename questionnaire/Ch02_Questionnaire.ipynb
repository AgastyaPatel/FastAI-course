{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXdty09SEaCnmDMCs5jIyc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 02 Questiopnnaire\n",
        "1. Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n",
        "\n",
        "2. Where do text models currently have a major deficiency?\n",
        "- Text models struggles with creating accurate reponses,\n",
        "- Text models can generate context-appropriate text like replies, mimimic author's writing style.\n",
        "- It is not yet reliable to create reponses based on a factual knowledge base however this seems issue seems to be solving with releases of llm like GPT, bard etc but these are still not 100% accurate.\n",
        "3. What are possible negative societal implications of text generation models?\n",
        "- Text generating abilities of a model might be used to create context aware, high compelling responses to spread misinformation at large scale.\n",
        "- Models are trained on bias data which might create bias output later leading to positive feedback loop.\n",
        "4. In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n",
        "- We can always have human supervision to evaluate the predictions from the model and determine the next step. In case of medical diagnoses, model could indentify the strokes in CT scan and alert high priority cases to be treated immediately. While other cases are sent to radiologist along with the outcomes of the model which can be helpful for radiologist and even help them identify symptoms that they would have missed.\n",
        "5. What kind of tabular data is deep learning particularly good at?\n",
        "- Deep learning is particularly good at analyzing tabular data that includes natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code)\n",
        "6. What's a key downside of directly using a deep learning model for recommendation systems?\n",
        "- Machine learning approaches for recommendation system will often tell what product that user might like and these recommendation may not be helpful to user.\n",
        "- System creates a recommendation to a user to purchase particular book from an author however user might have been familiar with the book or he might have already purchased the book in a book set which has been listed as a different product.\n",
        "7. What are the steps of the Drivetrain Approach?\n",
        "- Indentifing the objective\n",
        "- Plan actions which can be used to achieve those objectives\n",
        "- Gather data or can aquire them\n",
        "- Build a model that can be determine the best action to get the best result for the objective\n",
        "We use data not just to generate more data (in the form of predictions), but to produce actionable outcomes. That is the goal of the Drivetrain Approach\n",
        "8. How do the steps of the Drivetrain Approach map to a recommendation system?\n",
        "- Identify Objective: To drive additional sales by suprising and delighting the customer with the recommendation of item that they would have not purchased without the recommendation.\n",
        "- The lever/Action plan: The ranking of the product\n",
        "- Data: Collect data from random experiements in order to learn more about recommendation of products for wide range of customer.\n",
        "- Optimizing data to drive more sales\n",
        "9. Create an image recognition model using data you curate, and deploy it on the web.\n",
        "- Done\n",
        "10. What is DataLoaders?\n",
        "- Dataloaders is a container which contains the data (train and validation) we want to use to for our machine learning model; it organises the data.\n",
        "- DataLoaders class is the class that passes the data to the fastai model. It is essentially a class that required Dataloader objects (usually for train and validation subset)\n",
        "11. What four things do we need to tell fastai to create DataLoaders?\n",
        "- Kind of data that we are working with\n",
        "- how to get the list of items\n",
        "- how to label the items\n",
        "- Splitting ratio for training and validation set.\n",
        "12. What does the splitter parameter to DataBlock do?\n",
        "- Splitter arguement is used split the data into training and validation subset. We use RandomSplitter to split the data, and provide it with proportion (valid_pct) used for validation\n",
        "- Valid_pct: It denotes the ratio of the dataset which would be subsetted to be used as validation set\n",
        "13. How do we ensure a random split always gives the same validation set?\n",
        "Turns out computer can't really generate random numbers but they use pseudo-number generator seed. By seeting a random seed value, the pseudo-random generator will generate 'random' number in a fixed manner and it will be same for every run. Using seed we can generate subset that gives the same validation set always.\n",
        "14. What letters are often used to signify the independent and dependent variables?\n",
        "X is independent and y is dependent\n",
        "15. What's the difference between the crop, pad, and squish resize approaches?\n",
        "- Resize(size): crops the image to fit a square of the size requested, using the full width or height.\n",
        "- ResizeMethod.Squish: can result in loosing of some detail as images gets squished or stretched.\n",
        "- ResizeMethod.Pad, pad_mode='zero': Fits the image in square with black padding.\n",
        "16. When might you choose one over the others?\n",
        "- Which resizing method to use therefore depends on the underlying problem and dataset. For example, if the features in the dataset images take up the whole image and cropping may result in loss of information, squishing or padding may be more useful.\n",
        "- Another better method is to use RandomResizedCrop in which we crop on a randomly selected region of the image. So every epoch, the moel will see a different part of the image and will learn accordingly\n",
        "17. What is data augmentation? Why is it needed?\n",
        "- Process of creating random variation of the input data such that they appear little different but without changing the meaning of data. This can be done by changing the brightness, contrast, rotating the images, transforming them.\n",
        "- Data augmentation helps the model to better learn the basic features and concept of object structure instead of remembering them.\n",
        "- It allows the machine learning models to generalize the features.\n",
        "18. What is the difference between item_tfms and batch_tfms?\n",
        "- item_tfms are transformation applied to single data sample x on the CPU. Resize() is common transform because the mini batch of input images to a cnn must have same dimmensions.\n",
        "- batch_tfms are transformation applied to batched data samples (samples that have been collated into a mini-batch) on GPU. They are faster ad more efficient than item_tfms. A good example is aug_transforms()\n",
        "19. What is a confusion matrix?\n",
        "- Confusion matrix is a representation of the prediction made vs the correct labels. The rows of the matrrix represents the actual labels while the columns represents the predicted labels. The number of images in the diagonal element represents the correctly predicted labels while rest are incorrect.\n",
        "- It provides additional information about how is the performance of the model and where is the model getting confused.\n",
        "20. What does export save?\n",
        "- export saves both the architecture, as well as the trained parameters of the neural network architecture. It also saves how the DataLoaders are defined.\n",
        "- Vocab of the dataLoader\n",
        "21. What is it called when we use a model for getting predictions, instead of training?\n",
        "- Inference\n",
        "22. What are IPython widgets?\n",
        "- IPython widgets are javascript and python combined functionalities hat let us build and interact GUI component.\n",
        "23. When might you want to use CPU for deployment? When might GPU be better?\n",
        "- GPU are good for doing identical work in parallel. If we are performing small single task then it's better to use CPU as it would be cost effective with large number of market competition for CPU servers.\n",
        "- GPU could be used if we are batching several small task at a time, and perform inference on the whole batch. This may require user to wait for prediction as task can't start until batch is full. Additionally, there are other complexxities of managing GPU inference, like memory management and queuing of task.\n",
        "24. What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n",
        "- The application will require a network connection, there would be some latency in submitting and fetching result. Also, sending private data to network server can lead to security concers.\n",
        "- On other hand, maintaining of the model deployment, environment and rolling out of models would be easier as only one server needs to be modified.\n",
        "25. What are three examples of problems that could occur when rolling out a bear warning system in practice?\n",
        "- Handling low light images like night\n",
        "- Predictions are too slow to be useful\n",
        "- Handling low res pixelated images\n",
        "26. What is \"out-of-domain data\"?\n",
        "- Data which is different in some aspect to the data which was used for training. For example Object detection model was trained on photographs; and while using the model with hand drawn images provides poor performance. Here the hand drawn images are 'Out-Of-domain' data.\n",
        "27. What is \"domain shift\"?\n",
        "- Domain shift happens when type of data that model deals with keeps on changing with time.\n",
        "- Example, insurance company uses model as a part of its pricing and risk algorithm; however the risk and pricing depends on the types of customers that company attract and the risk that they represent. These may change over the perios\n",
        "28. What are the three steps in the deployment process?\n",
        "- Manual process: the model is run in parallel and not directly driving any actions, with humans still checking the model outputs.\n",
        "- Limited scope deployment: The model's scope is limited and carefully supervised. For example, doing a geographically and time-constrained trial of model deployment, that is carefully supervised.\n",
        "- Gradual expansion: The model scope is gradually increased, while good reporting systems are implemented in order to check for any significant changes to the actions taken compared to the manual process (i.e. the models should perform similarly to the humans, unless it is already anticipated to be better)."
      ],
      "metadata": {
        "id": "ay93_EDv7VWP"
      }
    }
  ]
}