{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aef4d8-f7ff-4149-857f-72c0abf706cf",
   "metadata": {},
   "source": [
    "# Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be60542-def8-43af-a965-1800c0b5c8d1",
   "metadata": {},
   "source": [
    "1. What is \"self-supervised learning\"?\n",
    "- Training a model without the use of labels or dependent variables.\n",
    "- Dependent variable is extracted implicitly from independent variables.\n",
    "2. What is a \"language model\"?\n",
    "- A language model is a self-supervised model which predicts the next word from the input text.\n",
    "3. Why is a language model considered self-supervised?\n",
    "- Language model is considered self-supervised model as there are no dependent/labels provided for training. Instead, the model learns to predict the next word by reading lots of provided text with no labels.\n",
    "4. What are self-supervised models usually used for?\n",
    "- Self-supervised models are used to detect the pattern and predict the next possible outcome in different task. But often, they are used as pre-trained models for transfer learning.\n",
    "5. Why do we fine-tune language models?\n",
    "- We fine-tune language model so that language model can understand the text context better and predict good quality results. Since the pre-trained model is trained on different corpus of text, fine-tuning would result in better understanding the of the new corpus of text.\n",
    "6. What are the three steps to create a state-of-the-art text classifier?\n",
    "- Train a language model on large corpus of text\n",
    "- Fine-tune the language model on text classification dataset\n",
    "- Fine-tune the language model as text classifier.\n",
    "7. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
    "- When unlabeled movie reviews is used to fine-tune language model. It helps the language model to understand the structure, language style of the movie review classification dataset. \n",
    "- Therefore, when fine-tuned as classifier.\n",
    "8. What are the three steps to prepare your data for a language model?\n",
    "- Tokenization\n",
    "- Numericalization\n",
    "- Language model DataLoader\n",
    "9. What is \"tokenization\"? Why do we need it?\n",
    "- Tokenization is the process of converting text into a list of words.\n",
    "- Tokenization needs to split words along with other characters like punctuation, accented characters and group of characters.\n",
    "10. Name three different approaches to tokenization.\n",
    "- Word-based tokenization\n",
    "- Subword Tokenization\n",
    "- Character Tokenization \n",
    "11. What is xxbos?\n",
    "- Special token added by fastai which represents beginning of the text.\n",
    "12. List four rules that fastai applies to text during tokenization.\n",
    "- fix_html: replaces special HTML characters by a readable version\n",
    "- replace_rep: replace any chaacter repeated three times or more by a special token for repetition(xxwrep)\n",
    "- replace_wrep: replace any word repeated three times or more by special token for word, Number of times it's repeated, then the word\n",
    "- spec_add_spaces: add spaces around / and #\n",
    "- rm_useless_spaces: remove all repetitions of space characters\n",
    "- replace_all_caps: lowecases a word written in all caps and adds a special token for all caps(xxcap) in front of it;\n",
    "- replace_maj: lowecase a capitalized word adds a special token for capitalized (xxmaj)\n",
    "- lowercase: lowercase all text and adds special token at the beginning xxbos and xxeos\n",
    "13. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
    "- We can expect that repeeated characteres could have special or different meaning than just a single character.\n",
    "14. What is \"numericalization\"?\n",
    "- This refers to the mapping of the tokens to integers to be passed into the model.\n",
    "15. Why might there be words that are replaced with the \"unknown word\" token?\n",
    "- If all the words in the dataset have a token associated with them, then the embedding matrix would be very large, increase memory usage and slow down training.\n",
    "- Therefore words with more than min_freq occurence are assigned a token and finally a number, while others are replaced with \"unknown word token\"\n",
    "\n",
    "16. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)\n",
    "- The dataset is split into 64 mini-streams (batch size)\n",
    "- Each batch has 64 rows (batch size) and 64 columns (sequence length)\n",
    "\n",
    "17. Why do we need padding for text classification? Why don't we need it for language modeling?\n",
    "\n",
    "18. What does an embedding matrix for NLP contain? What is its shape?\n",
    "\n",
    "19. What is \"perplexity\"?\n",
    "- Perplexity is a commonly used metric in NLP for language models. It is the exponential of the loss.\n",
    "20. Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
    "\n",
    "21. What is \"gradual unfreezing\"?\n",
    "\n",
    "22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875d818-313e-47eb-a20f-f2bfe0d92a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
