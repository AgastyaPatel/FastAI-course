{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8729c7-b219-4911-84ee-c8e7ab467e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor,nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadd9679-a438-4d29-98a1-a5fd299ad08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f62b6e-f94e-4af3-ba1a-43d126f77a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b04600-81d5-4842-8368-be6b756de0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856af000-be02-40e7-a8dc-a43edf0fa592",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb797b1-60b6-4345-b91d-2bcb6d975ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10), 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh=50\n",
    "n,m,c, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84acda3-6b20-496e-b0b5-825497d41ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1152786-d8ff-4499-8bca-8e4bcbf7cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.09, -0.21, -0.08,  ..., -0.03,  0.01,  0.06],\n",
       "        [-0.07, -0.14, -0.14,  ...,  0.03,  0.04,  0.14],\n",
       "        [-0.19, -0.04,  0.02,  ..., -0.01, -0.00,  0.02],\n",
       "        ...,\n",
       "        [-0.03, -0.22, -0.04,  ..., -0.01,  0.09,  0.14],\n",
       "        [-0.10, -0.09, -0.05,  ..., -0.01,  0.02,  0.11],\n",
       "        [-0.03, -0.25, -0.06,  ...,  0.00,  0.03,  0.14]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, 10)\n",
    "pred = model(x_train)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79abdac-76e9-4652-867c-eea2437db9be",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "First We will compute the softmax of the activation ie, softmax for output.\\\n",
    "Softmax for each activation: $\\frac{e^{activation}}{\\sum{e^{all_activations}}}$\n",
    "$$\\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b740fa6-702a-4b7e-a5fb-6833f7a12905",
   "metadata": {},
   "source": [
    "In practice, we will need the log of the softmax when we calculate the loss in order to get the cross entropy loss.\n",
    "Cross Entropy Loss Function$$-\\sum_{c=1}^My_{o,c}\\log(\\sigma(c_i))$$ ie, sum of output times the log of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a287638-fa83-4f82-9248-01b39e886278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1, keepdim=True))).log()\n",
    "# torch.exp() => e^x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18942d6c-203f-4a1b-a82f-975bafd99eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd047e-08e7-428c-b7e2-958af63a60ff",
   "metadata": {},
   "source": [
    "We know that, $\\log(\\frac{a}{b})= \\log(a) - \\log(b) $. This can be used to simplify the cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8feac918-6db4-42ed-8173-5644123d9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#therefore\n",
    "# def log_softmax(x): return x.exp().log() - (x.exp().sum(-1, keepdim=True)).log()\n",
    "# we know that log(e^x) = x\n",
    "def log_softmax(x): return x - (x.exp().sum(-1, keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e6e25-d870-4e0b-b0a9-8b4b2bc3c2f1",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp) trick. The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where a is the maximum of $x_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1e08c2-8d72-4605-ae9c-48cd7e5b5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15957a-17c2-4501-a0eb-7e502dc8e7a0",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In Pytorch, this is already implemented for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d075f8d0-724a-4fb7-93b8-67128f3edf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a17f915-d5ab-4112-95ba-4085aae70b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing custom version with pytorch implementation\n",
    "test_close(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "340464e8-4969-41f5-bb7d-5a5532d8bdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "         [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "         [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "         ...,\n",
       "         [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "         [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "         [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>),\n",
       " torch.Size([50000, 10]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred = log_softmax(pred)\n",
    "sm_pred, sm_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11866e23-5a90-4747-a22d-7db5d1f399ac",
   "metadata": {},
   "source": [
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ -\\sum x\\, \\log p(x) $$\n",
    "\n",
    "But since our $x$s are 1-hot encoded (actually, they're just the integer indices), this can be rewritten as $-\\log(p_{i})$ where i is the index of the desired target.\n",
    "\n",
    "This can be done using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f05a515-6cd0-4126-9e98-b73f793acf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6408b12b-0469-4259-8e71-9f24826ce5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.20, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.37, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.36, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the prediction index\n",
    "sm_pred[0, 5], sm_pred[1, 0], sm_pred[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716bc941-ec00-48d6-90a0-21e5be5ac4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.20, -2.37, -2.36], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], y_train[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998642f-a895-4369-a414-1e15b314a84f",
   "metadata": {},
   "source": [
    "#### Negative likelyhood loss\n",
    "log_softmax for hot encoded activations = $-\\log(p_{i})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6c8759d-2fe1-46c9-9c4f-721ebbd3b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(inp, target): return -inp[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d76e0e4-6619-4c2e-935a-cb0a6b48fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]), torch.Size([50000]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "402d3339-c541-41f1-a22d-19ce3f01cfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8290ad28-e164-41a0-bd7d-69a8f667e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54c31d8e-d2c0-44ce-ae3c-34d3321b916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
